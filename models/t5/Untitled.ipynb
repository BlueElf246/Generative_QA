{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b446c31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\env\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "Using custom data configuration default-2dd2a5479f3fb3f2\n",
      "Reusing dataset json (C:\\Users\\Admin\\.cache\\huggingface\\datasets\\json\\default-2dd2a5479f3fb3f2\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a93713f1a742cd8738e437d8b30f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "train_with_pytorch_lightning = True\n",
    "\n",
    "s2s_args = ArgumentsS2S(\n",
    "    batch_size= 16,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "path_train = \"C:/Users/Admin/Downloads/ELI5.jsonl\"\n",
    "path_val = \"C:/Users/Admin/Downloads/ELI5_val.jsonl\"\n",
    "\n",
    "# dataset_train = load_dataset('json', data_files = path_train)\n",
    "dataset_val = load_dataset('json', data_files = path_val)\n",
    "\n",
    "# train = dataset_train['train']\n",
    "val = dataset_val['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2d5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6debac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b35a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x000001DB84EA0CA0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c6076752e14cd095112ac8c689873f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1507 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train1 = train.map(lambda ex: preprocess(ex,num_docs=5), remove_columns = ['question_id'])\n",
    "val1 = val.map(lambda ex: preprocess(ex,num_docs=5), remove_columns = ['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159def4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sep_token = '<sep>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cc96fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['<sep>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f26f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dda8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='t5-base', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc62407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32101, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c332d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "def add_sep_token(example):\n",
    "    example['answers'] = \" <sep> \".join(example['answers'])\n",
    "    return example\n",
    "def add_eos_token(example):\n",
    "    example['ques_context'] =f\"question: {example['question']} context: {example['ctxs']} </s>\"\n",
    "    example['answers']=  example['answers'] + \" </s>\"\n",
    "    return example\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['ques_context'],\n",
    "                                                  max_length=max_input_length,\n",
    "                                                  add_special_tokens=True,\n",
    "                                                  truncation=True,\n",
    "                                                  pad_to_max_length=True)\n",
    "\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['answers'],\n",
    "                                                   max_length=max_target_length,\n",
    "                                                   add_special_tokens=True,\n",
    "                                                   truncation=True, pad_to_max_length=True)\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'decoder_input_ids': target_encodings['input_ids']\n",
    "        ,'decoder_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287ebddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94887cbbd3a64e16aa58f6e5a45489d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1507 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train2 = val1.map(add_sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b4d7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8c3bb107534be7930a81e095ca0c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1507 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train3 = train2.map(add_eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54022475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answers', 'ctxs', 'ques_context'],\n",
       "    num_rows: 1507\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8264d99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b2e3ed1c134bbb8c916f524fbf3bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train4 = train3.map(convert_to_features, batched=True, remove_columns=['question','answers','ctxs','ques_context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7764eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    EvalPrediction,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments)\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d450b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'decoder_input_ids', 'attention_mask', 'decoder_attention_mask']\n",
    "train4.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65ea441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T2TDataCollator():\n",
    "  def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Take a list of samples from a Dataset and collate them into a batch.\n",
    "    Returns:\n",
    "    A dictionary of tensors\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "    lm_labels = torch.stack([example['decoder_input_ids'] for example in batch])\n",
    "    lm_labels[lm_labels[:, :] == 0] = -100\n",
    "    attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "    decoder_attention_mask = torch.stack([example['decoder_attention_mask'] for example in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': lm_labels,\n",
    "        'decoder_attention_mask': decoder_attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f54db5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"model/\",\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  gradient_accumulation_steps=16,\n",
    "                                  learning_rate=1e-4,\n",
    "                                  num_train_epochs=7,\n",
    "                                  logging_steps=100,\n",
    "                                  run_name=\"QA_question_answering\",\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_steps=500,\n",
    "                                  optim='adamw_torch'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c9b3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\env\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/161 00:32 < 13:57, 0.18 it/s, Epoch 0.30/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train4,\n",
    "#     eval_dataset=valid_dataset,\n",
    "    data_collator=T2TDataCollator()\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc595a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
